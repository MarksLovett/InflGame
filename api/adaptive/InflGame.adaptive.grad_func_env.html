
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Adaptive Environment &#8212; InflGame  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/adaptive/InflGame.adaptive.grad_func_env';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Plots" href="InflGame.adaptive.visualization.html" />
    <link rel="prev" title="Adaptive (InflGame.adaptive)" href="InflGame.adaptive.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">InflGame  documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../InflGame.html">
    InflGame package
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../InflGame.html">
    InflGame package
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">User guide</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="InflGame.adaptive.html">Adaptive (<code class="xref py py-mod docutils literal notranslate"><span class="pre">InflGame.adaptive</span></code>)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Adaptive Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="InflGame.adaptive.visualization.html">Plots</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MARL/InflGame.MARL.html">Multi-Agent Learning (<code class="xref py py-mod docutils literal notranslate"><span class="pre">InflGame.MARL</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MARL/InflGame.MARL.async_game.html">Asynchronous Influencer Game</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MARL/InflGame.MARL.sync_game.html">Synchronous Influencer Game</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MARL/InflGame.MARL.IQL_async.html">Asynchronous IQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MARL/InflGame.MARL.IQL_sync.html">Synchronous IQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MARL/InflGame.MARL.MARL_plots.html">MARL Plots</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../MARL/InflGame.MARL.utils.html">MARL Utilities (<code class="xref py py-mod docutils literal notranslate"><span class="pre">MARL.utils</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../MARL/utils/InflGame.MARL.utils.experiments.html">MARL experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MARL/utils/InflGame.MARL.utils.IQL_utils.html">IQL Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MARL/utils/InflGame.MARL.utils.MARL_utils.html">General RL Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MARL/utils/InflGame.MARL.utils.my_parse.html">RAYRL Parser</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../domains/domains.html">Domains (<code class="xref py py-mod docutils literal notranslate"><span class="pre">InflGame.domains</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../domains/rd/rd.html">Resource Distributions</a></li>

<li class="toctree-l2 has-children"><a class="reference internal" href="../domains/one_d/one_d.html">One Dimensional Domains (<code class="xref py py-mod docutils literal notranslate"><span class="pre">domains.one_d</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../domains/one_d/plots.html">Plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="../domains/one_d/utils.html">Utilities</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../domains/two_d/two_d.html">Two Dimensional Domains (<code class="xref py py-mod docutils literal notranslate"><span class="pre">domains.two_d</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../domains/two_d/plots.html">Plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="../domains/two_d/utils.html">Utilities</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../domains/simplex/simplex.html">2D Simplex Domain (<code class="xref py py-mod docutils literal notranslate"><span class="pre">domains.simplex</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../domains/simplex/plots.html">Plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="../domains/simplex/utils.html">Utilities</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../kernels/InflGame.kernels.html">InflGame.kernels (<code class="xref py py-mod docutils literal notranslate"><span class="pre">InflGame.kernels</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../kernels/InflGame.kernels.gauss.html">Gaussian kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/InflGame.kernels.jones.html">Jones’ Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/InflGame.kernels.diric.html">Dirichlet Kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/InflGame.kernels.MV_gauss.html">Multi-Variate Gaussian Kernel</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../utils/InflGame.utils.html">InflGame.utils (<code class="xref py py-mod docutils literal notranslate"><span class="pre">InflGame.utils</span></code>)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../utils/InflGame.utils.general.html">General Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../utils/InflGame.utils.data_management.html">Data Management</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../InflGame.html" class="nav-link">InflGame package</a></li>
    
    
    <li class="breadcrumb-item"><a href="InflGame.adaptive.html" class="nav-link">Adaptive (<code class="xref py py-mod docutils literal notranslate"><span class="pre">InflGame.adaptive</span></code>)</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Adaptive Environment</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-InflGame.adaptive.grad_func_env">
<span id="adaptive-environment"></span><h1>Adaptive Environment<a class="headerlink" href="#module-InflGame.adaptive.grad_func_env" title="Link to this heading">#</a></h1>
<section id="adaptive-environment-module">
<span id="module-grad_func_env"></span><h2>Adaptive Environment Module<a class="headerlink" href="#adaptive-environment-module" title="Link to this heading">#</a></h2>
<p>This module defines the <em class="xref py py-obj">AdaptiveEnv</em> class, which represents an adaptive environment for agents interacting with a resource distribution via influence kernels.
The class models the competition of agents via their influence over the environment and computes gradients for optimization. 
It provides methods to compute influence, reward, and the gradients of rewards of each agent.</p>
<p>The module supports different types of influence kernels, including:
- Gaussian
- Jones
- Dirichlet
- Multi-variate Gaussian
- Custom influence kernels (user-defined)</p>
<p>The <em class="xref py py-obj">AdaptiveEnv</em> class supports gradient ascent methods for optimizing agent positions in the environment.</p>
<section id="dependencies">
<h3>Dependencies:<a class="headerlink" href="#dependencies" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>InflGame.utils</p></li>
<li><p>InflGame.kernels</p></li>
<li><p>InflGame.domains</p></li>
</ul>
</section>
<section id="usage">
<h3>Usage:<a class="headerlink" href="#usage" title="Link to this heading">#</a></h3>
<p>The <em class="xref py py-obj">AdaptiveEnv</em> class can be used to simulate and optimize agent interactions in an environment with resource distributions. It supports various influence kernel types and gradient ascent methods for optimization.</p>
</section>
<section id="example">
<h3>Example:<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">InflGame.adaptive.grad_func_env</span> <span class="kn">import</span> <span class="n">AdaptiveEnv</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Initialize the environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">AdaptiveEnv</span><span class="p">(</span>
    <span class="n">num_agents</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">agents_pos</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]),</span>
    <span class="n">parameters</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span>
    <span class="n">resource_distribution</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">]),</span>
    <span class="n">bin_points</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]),</span>
    <span class="n">infl_configs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;infl_type&#39;</span><span class="p">:</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">},</span>
    <span class="n">learning_rate_type</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span>
    <span class="n">time_steps</span><span class="o">=</span><span class="mi">100</span>
    <span class="n">domain_type</span><span class="o">=</span><span class="s1">&#39;1d&#39;</span><span class="p">,</span>
    <span class="n">domain_bounds</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Perform gradient ascent</span>
<span class="n">env</span><span class="o">.</span><span class="n">gradient_ascent</span><span class="p">(</span><span class="n">show_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<p class="rubric">Classes</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table">
<tbody>
</tbody>
</table>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">InflGame.adaptive.grad_func_env.</span></span><span class="sig-name descname"><span class="pre">AdaptiveEnv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_agents</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agents_pos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resource_distribution</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bin_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">infl_configs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{'infl_type':</span> <span class="pre">'gaussian'}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cosine'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[0.0001,</span> <span class="pre">0.01,</span> <span class="pre">15]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">infl_cshift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cshift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">infl_fshift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Q</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'1d'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain_bounds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[0,</span> <span class="pre">1]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tolerance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tolerated_agents</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_zero_infl</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv" title="Link to this definition">#</a></dt>
<dd><p>The AdaptiveEnv class represents an adaptive environment for agents interacting with a resource distribution via influence kernels.
This class models the competition of agents via their influence over the environment and computes gradients for optimization. 
The class provides methods to compute influence, reward, and gradients based on the influence of agents on resource distribution.
It also supports different types of influence kernels, including Gaussian, Jones, Dirichlet, and custom influence kernels.</p>
<p class="rubric">Methods</p>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">d_lnf_matrix</span></code></a>(parameter_instance)</p></td>
<td><p>Computes the derivative of the log of the influence function matrix , i.e. .</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">d_torch</span></code></a>(parameter_instance)</p></td>
<td><p>Compute the gradient of the custom influence matrix using PyTorch autograd i.e.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gradient</span></code></a>(parameter_instance)</p></td>
<td><p>Compute the gradient of the reward function <span class="math notranslate nohighlight">\(u_i(x)\)</span> with respect to agent positions <em class="xref py py-obj">x_i</em>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_ascent" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_ascent"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gradient_ascent</span></code></a>([show_out, grad_modify, reward])</p></td>
<td><p>This is the helper function for performing gradient ascent for agents in the environment.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_function" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_function"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gradient_function</span></code></a>(agents_pos, parameter_instance)</p></td>
<td><p>The gradient function computes the gradient of the reward function for a given set of agent positions and parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.influence_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.influence_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">influence_matrix</span></code></a>(parameter_instance)</p></td>
<td><p>Compute the influence matrix for all agents using vectorized operations.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.mv_gradient_ascent" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.mv_gradient_ascent"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mv_gradient_ascent</span></code></a>([show_out, grad_modify, ...])</p></td>
<td><p>Perform multi-variable gradient ascent for agents in the environment using the graident calculated by the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prob_matrix</span></code></a>(parameter_instance)</p></td>
<td><p>Computes the probability matrix for agents influencing a resource based on their influence kernel :math:'f_{i}(x_i,b_k)` computed by <code class="xref py py-func docutils literal notranslate"><span class="pre">influence</span></code> .</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.reward_F" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.reward_F"><code class="xref py py-obj docutils literal notranslate"><span class="pre">reward_F</span></code></a>(parameter_instance)</p></td>
<td><p>Compute the expected reward for each agent given a reward distribution and all agents influence kernels.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.shift_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.shift_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shift_matrix</span></code></a>(parameter_instance)</p></td>
<td><p>Compute the shift matrix for functional shifts in influence kerenels.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.sv_gradient_ascent" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.sv_gradient_ascent"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sv_gradient_ascent</span></code></a>([show_out, grad_modify, ...])</p></td>
<td><p>The gradient ascent is performed by updating the agent positions based on the gradient and a learning rate.</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix">
<span class="sig-name descname"><span class="pre">d_lnf_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix" title="Link to this definition">#</a></dt>
<dd><p>Computes the derivative of the log of the influence function matrix , i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial}{\partial x_{(i,l)}}ln(f_{i}(x_i,b))=\frac{1}{f_{i}(x_{i},b)}\frac{\partial}{\partial x_{(i,l)}}f_{i}(x_{i},b)\]</div>
</div></blockquote>
<p>The derivative matrix is a <span class="math notranslate nohighlight">\(N \times K\)</span> matrix where <span class="math notranslate nohighlight">\(N\)</span> is the number of agents and K is the number of bin/resource points.
The entry <span class="math notranslate nohighlight">\(\frac{\partial}{\partial x_i}ln(f_{i}(x_i,b_k))\)</span> is a the derivative of the log of the influence of the <span class="math notranslate nohighlight">\(i\)</span> th agent on the <span class="math notranslate nohighlight">\(k\)</span> th bin/resource point.
i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\mathbf{D}=\begin{bmatrix}
\frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_1)) &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_K)) \\
\frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_1)) &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_K)) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_1)) &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_K))
\end{bmatrix}\end{split}\]</div>
</div></blockquote>
<p>This function <strong>only</strong> used for the <strong>prebuilt</strong> influence kernels from the paper in <code class="xref py py-func docutils literal notranslate"><span class="pre">influence</span></code> where the derivatives are analytically computed.
-<strong>Gaussian influence kernel</strong></p>
<blockquote>
<div><p>(infl_type==’gaussian’)</p>
</div></blockquote>
<ul>
<li><p><strong>Jones influence kernel</strong></p>
<blockquote>
<div><p>(infl_type==’Jones_M’)</p>
</div></blockquote>
</li>
<li><p><strong>Dirichlet influence kernel</strong></p>
<blockquote>
<div><p>(infl_type==’dirichlet’)</p>
</div></blockquote>
</li>
<li><p><strong>Multi-variate Gaussian influence kernel</strong></p>
<blockquote>
<div><p>(infl_type==’multi_gaussian’)</p>
</div></blockquote>
</li>
</ul>
<p><strong>For custom influence kernels</strong> use <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">d_torch</span></code></a>. This is automatically done if infl_type==custom_influence by the adaptive_env class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Derivative matrix.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[int, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch">
<span class="sig-name descname"><span class="pre">d_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch" title="Link to this definition">#</a></dt>
<dd><p>Compute the gradient of the custom influence matrix using PyTorch autograd i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial}{\partial x_{(i,l)}}ln(f_{i}(x_i,b))=\frac{1}{f_{i}(x_{i},b)}\frac{\partial}{\partial x_{(i,l)}}f_{i}(x_{i},b)\]</div>
</div></blockquote>
<p>if you using the infl_type=’custom’ influence kernel. This is done using PyTorch’s autograd functionality, so number of bin points must be larger enough to compute the gradient ( <span class="math notranslate nohighlight">\(K\sim 100\)</span> ).</p>
<p>If you are using a non-custom influence kernel, use <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">d_lnf_matrix</span></code></a> instead, this is automatically done by the adaptive_env class.</p>
<p>The derivative matrix is a <span class="math notranslate nohighlight">\(N \times K\)</span> matrix where <span class="math notranslate nohighlight">\(N\)</span> is the number of agents and K is the number of bin/resource points.
The entry <span class="math notranslate nohighlight">\(\frac{\partial}{\partial x_i}ln(f_{i}(x_i,b_k))\)</span> is a the gradient of the log of the influence of the <span class="math notranslate nohighlight">\(i\)</span> th agent on the <span class="math notranslate nohighlight">\(k\)</span> th bin/resource point.
i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\mathbf{D}=\begin{bmatrix}
\frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_1)) &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_K)) \\
\frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_1)) &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_K)) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_1)) &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_K))
\end{bmatrix}\end{split}\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>derivative matrix.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input parameters are invalid or incompatible.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
<li><p><strong>NotImplementedError</strong> – If custom influence function is not properly configured.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient">
<span class="sig-name descname"><span class="pre">gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient" title="Link to this definition">#</a></dt>
<dd><p>Compute the gradient of the reward function <span class="math notranslate nohighlight">\(u_i(x)\)</span> with respect to agent positions <em class="xref py py-obj">x_i</em>.
The gradient is computed as the elment-wise product of the derivative of the log of the influence function matrix and the probability matrix dot-producted with the resource vector
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span> . 
i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial x_{(i,l)}}u_i(x)=\sum_{k=1}^{K}G_{i,k}(x_i,b_k)\frac{\partial}{\partial x_{(i,l)}}ln(f_{i}(x_i,b_k))\\
=\left(\mathbf{G}\odot\mathbf{D}\right) \cdot \vec{B}\\\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla\vec{R}=\left(\begin{bmatrix}
G_{1,1} &amp; G_{1,2} &amp; \cdots &amp; G_{1,K} \\
G_{2,1} &amp; G_{2,2} &amp; \cdots &amp; G_{2,K} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
G_{N,1} &amp; G_{N,2} &amp; \cdots &amp; G_{N,K}
\end{bmatrix}
\odot
\begin{bmatrix}
\frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_1)) &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_K)) \\
\frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_1)) &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_K)) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_1)) &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_K))
\end{bmatrix}
\right)\cdot 
\begin{bmatrix}
B_1 \\
B_2 \\
\vdots \\
B_K
\end{bmatrix}\end{split}\]</div>
</div></blockquote>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is the derivative of the log of the influence function matrix computed by <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">d_lnf_matrix</span></code></a> or <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">d_torch</span></code></a> .
The probability matrix <span class="math notranslate nohighlight">\(\mathbf{G}\)</span> is computed by the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">prob_matrix</span></code></a>.</p>
<p>The output <span class="math notranslate nohighlight">\(\nabla\vec{R}\)</span> is a <span class="math notranslate nohighlight">\(N \times L\)</span> matrix where <span class="math notranslate nohighlight">\(N\)</span> is the number of agents and <span class="math notranslate nohighlight">\(L\)</span> is the number of dimensions.
The entry <span class="math notranslate nohighlight">\(\nabla\vec{R}_{i,l}\)</span> is a the gradient of the reward of the <span class="math notranslate nohighlight">\(i\)</span> th agent on the <span class="math notranslate nohighlight">\(l\)</span> th dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradient values.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input parameters are invalid or incompatible.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_ascent">
<span class="sig-name descname"><span class="pre">gradient_ascent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">show_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_modify</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_ascent" title="Link to this definition">#</a></dt>
<dd><p>This is the helper function for performing gradient ascent for agents in the environment. It calls the appropriate gradient ascent function based on the domain type.
The gradient ascent is performed using the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.sv_gradient_ascent" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.sv_gradient_ascent"><code class="xref py py-func docutils literal notranslate"><span class="pre">sv_gradient_ascent</span></code></a> or <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.mv_gradient_ascent" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.mv_gradient_ascent"><code class="xref py py-func docutils literal notranslate"><span class="pre">mv_gradient_ascent</span></code></a> depending on the domain type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>show_out</strong> (<em>bool</em>) – Whether to return intermediate outputs.</p></li>
<li><p><strong>grad_modify</strong> (<em>bool</em>) – Whether to modify gradients.</p></li>
<li><p><strong>reward</strong> (<em>bool</em>) – Whether to compute rewards.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradient ascent results.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input parameters are invalid or incompatible.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
<li><p><strong>AttributeError</strong> – If required attributes are missing from the environment.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_function">
<span class="sig-name descname"><span class="pre">gradient_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">agents_pos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[0,</span> <span class="pre">1]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">two_a</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_function" title="Link to this definition">#</a></dt>
<dd><p>The gradient function computes the gradient of the reward function for a given set of agent positions and parameters. 
It is used to compute the gradient of the reward function for a specific set of agents, given a postion vector and parameters.
The gradient is computed as the elment-wise product of the derivative of the log of the influence function matrix and the probability matrix dot-producted with the resource vector
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span> .</p>
<p>i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial x_{(i,l)}}u_i(x)=\sum_{b\in \mathbb{B}}^{K}B(b)G_{i}(x_i,b)\\
=\sum_{k=1}^{K}G_{i,k}(x_i,b_k)\frac{\partial}{\partial x_{(i,l)}}ln(f_{i}(x_i,b_k))\\
=\left(\mathbf{G}\odot\mathbf{D}\right) \cdot \vec{B}\\\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla\vec{R}=\left(\begin{bmatrix}
G_{1,1} &amp; G_{1,2} &amp; \cdots &amp; G_{1,K} \\
G_{2,1} &amp; G_{2,2} &amp; \cdots &amp; G_{2,K} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
G_{N,1} &amp; G_{N,2} &amp; \cdots &amp; G_{N,K}
\end{bmatrix}
\odot
\begin{bmatrix}
\frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_1)) &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_1}ln(f_{1}(x_1,b_K)) \\
\frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_1)) &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_2}ln(f_{2}(x_2,b_K)) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_1)) &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_2)) &amp; \cdots &amp; \frac{\partial}{\partial x_N}ln(f_{N}(x_N,b_K))
\end{bmatrix}
\right)\cdot
\begin{bmatrix}
B_1 \\
B_2 \\
\vdots \\
B_K
\end{bmatrix}\end{split}\]</div>
</div></blockquote>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is the derivative of the log of the influence function matrix computed by <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">d_lnf_matrix</span></code></a> or <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">d_torch</span></code></a> .
The probability matrix <span class="math notranslate nohighlight">\(\mathbf{G}\)</span> is computed by the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">prob_matrix</span></code></a>.</p>
<p>The output <span class="math notranslate nohighlight">\(\nabla\vec{R}\)</span> is a <span class="math notranslate nohighlight">\(N \times L\)</span> matrix where <span class="math notranslate nohighlight">\(N\)</span> is the number of agents and <span class="math notranslate nohighlight">\(L\)</span> is the number of dimensions.
The entry <span class="math notranslate nohighlight">\(\nabla\vec{R}_{i,l}\)</span> is a the gradient of the reward of the <span class="math notranslate nohighlight">\(i\)</span> th agent on the <span class="math notranslate nohighlight">\(l\)</span> th dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>agents_pos</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>]</em>) – Positions of the agents.</p></li>
<li><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p></li>
<li><p><strong>ids</strong> (<em>List</em><em>[</em><em>int</em><em>]</em>) – IDs of the agents to compute gradients for.</p></li>
<li><p><strong>two_a</strong> (<em>bool</em>) – Whether to compute gradients for all agents.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradient values.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input parameters are invalid or incompatible.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.influence_matrix">
<span class="sig-name descname"><span class="pre">influence_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.influence_matrix" title="Link to this definition">#</a></dt>
<dd><p>Compute the influence matrix for all agents using vectorized operations.</p>
<p>This function computes the influence values for all agents across all bin points,
with optional constant and functional shifts. The function supports multiple
influence kernel types and provides comprehensive error handling.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Influence matrix of shape (N, K) or (N+shifts, K) where N is number of agents,
K is number of bin points, and shifts are additional rows for constant/functional shifts.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input parameters are invalid or incompatible.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
<li><p><strong>NotImplementedError</strong> – If functional shift is requested for multi-dimensional agents.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.mv_gradient_ascent">
<span class="sig-name descname"><span class="pre">mv_gradient_ascent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">show_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_modify</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.mv_gradient_ascent" title="Link to this definition">#</a></dt>
<dd><p>Perform multi-variable gradient ascent for agents in the environment using the graident calculated by the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient</span></code></a>.</p>
<p>The gradient ascent is performed by updating the agent positions based on the gradient and a learning rate. The learning rate is scheduled using the function <a class="reference internal" href="../utils/InflGame.utils.general.html#InflGame.utils.general.learning_rate" title="InflGame.utils.general.learning_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">InflGame.utils.general.learning_rate</span></code></a>.
The alogrithm is performed for a fixed number of time steps or until the agents converge to a solution within a specified tolerance for the absoulte difference between the current and previous agent positions.
The gradient ascent is performed in the following steps:</p>
<ol class="arabic">
<li><p>Compute the gradient of the reward function using the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient</span></code></a>.</p></li>
<li><p>Normalize the gradient if the domain type is ‘simplex’.</p>
<blockquote>
<div><ul class="simple">
<li><p>For simplex, the gradient is normalized to ensure that the agent positions remain within the simplex.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Update the agent positions using the gradient and the learning rate.</p>
<blockquote>
<div><ul class="simple">
<li><p>The learning rate is computed using the function <a class="reference internal" href="../utils/InflGame.utils.general.html#InflGame.utils.general.learning_rate" title="InflGame.utils.general.learning_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">InflGame.utils.general.learning_rate</span></code></a>.</p></li>
<li><p>The agent positions are updated by adding the gradient multiplied by the learning rate to the current agent positions.</p></li>
<li><p>The updated agent positions are projected onto the simplex if the domain type is ‘simplex’.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Store the agent positions, gradients, and rewards at each time step.</p></li>
<li><p>Check for convergence by computing the absolute difference between the current and previous agent positions.</p></li>
<li><p>If the absolute difference is less than the specified tolerance (var:tolerance) for a set number of agents (var:tolarated_agents), break the loop.</p></li>
</ol>
<p>i.e. a time step looks liek this:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x}_{i;t+1}=\mathbf{x}_{i;t}+\eta_t\cdot\nabla\vec{R}_{i;t}\\\end{split}\]</div>
</div></blockquote>
<p>with the stop condition:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\sum_{i=1}^{N}||\mathbf{x}_{i;t+1}-\mathbf{x}_{i;t}||_1\leq \epsilon = E\\\end{split}\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is the tolerance and <span class="math notranslate nohighlight">\(E\)</span> is the tolerated agents.
The learning rate <span class="math notranslate nohighlight">\(\eta_t\)</span> is computed using the function <a class="reference internal" href="../utils/InflGame.utils.general.html#InflGame.utils.general.learning_rate" title="InflGame.utils.general.learning_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">InflGame.utils.general.learning_rate</span></code></a>.</p>
<blockquote>
<div><p>If the domain type is ‘simplex’, the agent positions are projected onto the simplex so the update step looks like this:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x}_{i;t+1}=\mathbf{P}_{\Delta}(\mathbf{x}_{i;t}+\eta_t\cdot normalized(\nabla \vec{R}_{i;t}))\\\end{split}\]</div>
</div></blockquote>
<p>using the function <a class="reference internal" href="../domains/simplex/utils.html#InflGame.domains.simplex.simplex_utils.projection_onto_simplex" title="InflGame.domains.simplex.simplex_utils.projection_onto_simplex"><code class="xref py py-func docutils literal notranslate"><span class="pre">InflGame.domains.simplex.simplex_utils.projection_onto_simplex</span></code></a>.</p>
<p>Due to the normalization of the gradient, the stoping condtions is slightly different:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\sum_{i=1}^{N}||\mathbf{x}_{i;t+5}-\mathbf{x}_{i;t}||_1\leq \epsilon = E\\\end{split}\]</div>
</div></blockquote>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>show_out</strong> (<em>bool</em>) – Whether to return intermediate outputs.</p></li>
<li><p><strong>grad_modify</strong> (<em>bool</em>) – Whether to modify gradients.</p></li>
<li><p><strong>reward</strong> (<em>bool</em>) – Whether to compute rewards.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradient ascent results.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input parameters are invalid or incompatible.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix">
<span class="sig-name descname"><span class="pre">prob_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix" title="Link to this definition">#</a></dt>
<dd><p>Computes the probability matrix for agents influencing a resource based on their influence kernel :math:’f_{i}(x_i,b_k)` computed by <code class="xref py py-func docutils literal notranslate"><span class="pre">influence</span></code> . 
Where the probability of agent <span class="math notranslate nohighlight">\(i\)</span> influencing a bin/resource point is defined as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[G_{i,k}(\mathbf{x},b_k)=\frac{f_{i}(x_i,b_k)}{\sum_{j=1}^{N}f_{j}(x_j,b_k)}.\]</div>
</div></blockquote>
<p>The probability matrix is a <span class="math notranslate nohighlight">\(N \times K\)</span> matrix where <span class="math notranslate nohighlight">\(N\)</span> is the number of agents and K is the number of bin/resource points. 
The entry <span class="math notranslate nohighlight">\(G_{i,k}\)</span> is a the probability of the <span class="math notranslate nohighlight">\(i\)</span> th agent on the <span class="math notranslate nohighlight">\(k\)</span> th bin/resource point.
i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
G_{1,1} &amp; G_{1,2} &amp; \cdots &amp; G_{1,K} \\
G_{2,1} &amp; G_{2,2} &amp; \cdots &amp; G_{2,K} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
G_{N,1} &amp; G_{N,2} &amp; \cdots &amp; G_{N,K}
\end{bmatrix}\end{split}\]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Probability matrix.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input dimensions are incompatible or contain invalid values.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.reward_F">
<span class="sig-name descname"><span class="pre">reward_F</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.reward_F" title="Link to this definition">#</a></dt>
<dd><p>Compute the expected reward for each agent given a reward distribution and all agents influence kernels. The probability of an agent influencing a point is their relative influence over the bin points.</p>
<p>Then reward is computed as matrix multiplication of the probability matrix and the resource distribution.
i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}u_i&amp;=\sum_{b\in \mathbb{B}}^{K}B(b) G_{i}(x_i,b))\\
&amp;=\sum_{k=1}^{K}B_k G_{i,k}(x_i,b_k)\\
&amp;=\begin{bmatrix}
G_{1,1} &amp; G_{1,2} &amp; \cdots &amp; G_{1,K} \\
G_{2,1} &amp; G_{2,2} &amp; \cdots &amp; G_{2,K} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
G_{N,1} &amp; G_{N,2} &amp; \cdots &amp; G_{N,K}
\end{bmatrix}
\begin{bmatrix}
B_1 \\
B_2 \\
\vdots \\
B_K
\end{bmatrix}\end{split}\]</div>
</div></blockquote>
<p>if <span class="math notranslate nohighlight">\(\mathbb{B}=\set{b_1,b_2,\cdots,b_K}\)</span> is the set of bin points and <span class="math notranslate nohighlight">\(B_k=B(b_k)\)</span> is the resource at bin point <span class="math notranslate nohighlight">\(b_k\)</span>.</p>
<p>The probability matrix is computed by the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">prob_matrix</span></code></a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Reward values for agents.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input dimensions are incompatible or contain invalid values.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.shift_matrix">
<span class="sig-name descname"><span class="pre">shift_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_instance</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.shift_matrix" title="Link to this definition">#</a></dt>
<dd><p>Compute the shift matrix for functional shifts in influence kerenels. This function is mostly used for abstaining voters and fixed party examples, but can also be used for 
demonstrating how types of non-symmetry can impact agent behavior. 
The shift matrix is a <span class="math notranslate nohighlight">\(N \times K\)</span> matrix where <span class="math notranslate nohighlight">\(N\)</span> is the number of agents and K is the number of bin/resource points.
The entry <span class="math notranslate nohighlight">\(s_{i,k}\)</span> is a the shift of the <span class="math notranslate nohighlight">\(i\)</span> th agent on the <span class="math notranslate nohighlight">\(k\)</span> th bin/resource point.
i.e.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
s_{1,1} &amp; s_{1,2} &amp; \cdots &amp; s_{1,K} \\
s_{2,1} &amp; s_{2,2} &amp; \cdots &amp; s_{2,K} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
s_{N,1} &amp; s_{N,2} &amp; \cdots &amp; s_{N,K}
\end{bmatrix}\end{split}\]</div>
</div></blockquote>
<p>there are different types of shifts that can be applied to the influence matrix.</p>
<ul>
<li><p><strong>Constant shift</strong> (infl_cshift=True)</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[s_{i,k}=cshift\]</div>
</div></blockquote>
</li>
<li><dl>
<dt><strong>Functional shift</strong> (infl_fshift=True)’</dt><dd><p>An example of a functional shift is the abstaining voter model where the shift is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}s_{i,k}=-2Q\prod_{\substack{j=1\\ j\neq i}}^{N} (b_k-x_j)^2(b_k-x_i)\end{split}\]</div>
<p>i.e. the influence of an abstaining voter on point <span class="math notranslate nohighlight">\(b_k\)</span> is</p>
<div class="math notranslate nohighlight">
\[s_{i}(x_i,b_k)=\prod_{i=1}^{N} Q(b_k-x_j)^2\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is the position of the <span class="math notranslate nohighlight">\(i\)</span> th agent and <span class="math notranslate nohighlight">\(b_k\)</span> is the <span class="math notranslate nohighlight">\(k\)</span> th bin point.
<span class="math notranslate nohighlight">\(Q\)</span> is a scaling factor for the functional shift.</p>
</dd>
</dl>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>parameter_instance</strong> (<em>Union</em><em>[</em><em>List</em><em>[</em><em>float</em><em>]</em><em>, </em><em>np.ndarray</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Parameters for the influence kernels.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Shift matrix.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input dimensions are incompatible or invalid.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="InflGame.adaptive.grad_func_env.AdaptiveEnv.sv_gradient_ascent">
<span class="sig-name descname"><span class="pre">sv_gradient_ascent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">show_out</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_modify</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.sv_gradient_ascent" title="Link to this definition">#</a></dt>
<dd><p>The gradient ascent is performed by updating the agent positions based on the gradient and a learning rate. The learning rate is scheduled using the function <a class="reference internal" href="../utils/InflGame.utils.general.html#InflGame.utils.general.learning_rate" title="InflGame.utils.general.learning_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">InflGame.utils.general.learning_rate</span></code></a>.
The alogrithm is performed for a fixed number of time steps or until the agents converge to a solution within a specified tolerance for the absoulte difference between the current and previous agent positions.
The gradient ascent is performed in the following steps:</p>
<ol class="arabic">
<li><p>Compute the gradient of the reward function using the function <a class="reference internal" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient" title="InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient"><code class="xref py py-func docutils literal notranslate"><span class="pre">gradient</span></code></a>.</p></li>
<li><p>Update the agent positions using the gradient and the learning rate.</p>
<blockquote>
<div><ul class="simple">
<li><p>The learning rate is computed using the function <a class="reference internal" href="../utils/InflGame.utils.general.html#InflGame.utils.general.learning_rate" title="InflGame.utils.general.learning_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">InflGame.utils.general.learning_rate</span></code></a>.</p></li>
<li><p>The agent positions are updated by adding the gradient multiplied by the learning rate to the current agent positions.</p></li>
</ul>
</div></blockquote>
</li>
</ol>
<ol class="arabic simple" start="4">
<li><p>Store the agent positions, gradients, and rewards at each time step.</p></li>
<li><p>Check for convergence by computing the absolute difference between the current and previous agent positions.</p></li>
<li><p>If the absolute difference is less than the specified tolerance (var:tolerance) for a set number of agents (var:tolarated_agents), break the loop.</p></li>
</ol>
<p>i.e. a time step looks liek this:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\mathbf{x}_{i;t+1}=\mathbf{x}_{i;t}+\eta_t\cdot\nabla\vec{R}_{i;t}\\\end{split}\]</div>
</div></blockquote>
<p>with the stop condition:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\sum_{i=1}^{N}||\mathbf{x}_{i;t+1}-\mathbf{x}_{i;t}||_1\leq \epsilon = E\\\end{split}\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is the tolerance and <span class="math notranslate nohighlight">\(E\)</span> is the tolerated agents.
The learning rate <span class="math notranslate nohighlight">\(\eta_t\)</span> is computed using the function <a class="reference internal" href="../utils/InflGame.utils.general.html#InflGame.utils.general.learning_rate" title="InflGame.utils.general.learning_rate"><code class="xref py py-func docutils literal notranslate"><span class="pre">InflGame.utils.general.learning_rate</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>show_out</strong> (<em>bool</em>) – Whether to return intermediate outputs.</p></li>
<li><p><strong>grad_modify</strong> (<em>bool</em>) – Whether to modify gradients.</p></li>
<li><p><strong>reward</strong> (<em>bool</em>) – Whether to compute rewards.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Gradient ascent results.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If input parameters are invalid or incompatible.</p></li>
<li><p><strong>RuntimeError</strong> – If computation fails due to numerical issues.</p></li>
<li><p><strong>TypeError</strong> – If input types are not supported.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="InflGame.adaptive.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Adaptive (<code class="xref py py-mod docutils literal notranslate"><span class="pre">InflGame.adaptive</span></code>)</p>
      </div>
    </a>
    <a class="right-next"
       href="InflGame.adaptive.visualization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Plots</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-environment-module">Adaptive Environment Module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dependencies">Dependencies:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#usage">Usage:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_lnf_matrix"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.d_lnf_matrix</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.d_torch"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.d_torch</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.gradient</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_ascent"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.gradient_ascent</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.gradient_function"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.gradient_function</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.influence_matrix"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.influence_matrix</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.mv_gradient_ascent"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.mv_gradient_ascent</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.prob_matrix"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.prob_matrix</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.reward_F"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.reward_F</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.shift_matrix"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.shift_matrix</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#InflGame.adaptive.grad_func_env.AdaptiveEnv.sv_gradient_ascent"><code class="docutils literal notranslate"><span class="pre">AdaptiveEnv.sv_gradient_ascent</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/api/adaptive/InflGame.adaptive.grad_func_env.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025, Mark Lovett.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>